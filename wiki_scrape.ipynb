{
 "metadata": {
  "name": "wiki_scrape"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Forecasting the Papal Conclave"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Based off the great work of David Masad. [@badnetworker](http://twitter.com/badnetworker) [His notebook](http://nbviewer.ipython.org/urls/raw.github.com/dmasad/Pope_Forecasting/master/Conclave_Modeling_Tutorial.ipynb).\n",
      "* Forecast how the Cardinals will vote for the next Pope\n",
      "* Assumption being that they will vote for the candidate most like themselves\n",
      "* Determine similarity of Cardinals by the \"distance\" between their wikipedia page\n",
      "  - Determining [similarity of text](http://en.wikipedia.org/wiki/Category:String_similarity_measures) is a common task in computational science"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Useful Links"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Dive into Python](http://www.diveintopython.net/) is a fantastic resource.\n",
      "\n",
      "Modules needed:\n",
      "\n",
      "* urllib2\n",
      "* lxml\n",
      "* urlencode\n",
      "* json"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "URL = \"http://en.wikipedia.org/wiki/Cardinal_electors_in_Papal_conclave,_2005\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[http://en.wikipedia.org/wiki/Cardinal_electors_in_Papal_conclave,_2005](http://en.wikipedia.org/wiki/Cardinal_electors_in_Papal_conclave,_2005)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Requests"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "What not to do"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import urllib\n",
      "#url = \"http://en.wikipedia.org/wiki/Cardinal_electors_in_Papal_conclave,_2005\"\n",
      "#page = urllib.urlopen(url).read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Don't do this. It's inefficient, and it's rude.\n",
      "* Instead you want to send HTTP `Requests`\n",
      "* To do this, we'll use `urllib2`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* First create a `Request` object.\n",
      "* Nothing is retrieved here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "request = urllib2.Request(URL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We will want to add `request headers`\n",
      "* We could pass these to `Request` as a `dictionary`, though we will do so afterward below"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* First we will define a `User-Agent`\n",
      "* A `User-Agent` lets the server know who the client is that's requesting a page and how to get in touch if something is amiss"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "request.add_header(\"User-Agent\", \n",
      "                   \"WikiApiDemo/0.0 +http://jseabold.net\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* There are other headers you could define, but this is enough for our purposes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Retrieve the URL"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The next step is to build an opener. \n",
      "  - This is optional but will allow us to handle redirects and errors though we do not here"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "opener = urllib2.build_opener()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "page = opener.open(request)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "page.headers.dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Aside:** If you are building a client that may request the same page many times, you should look into `httplib2` which supports caching out of the box. It is also really easy to implement whatever custom backend for caching that you want."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Parsing HTML"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* There are a few popular choices for parsing HTML, depending on what you need\n",
      "* There are a few [built-in options](http://docs.python.org/2/library/markup.html) like `HTMLParser` and `xml` modules\n",
      "* There is the third-party [`lxml`](http://lxml.de/) library\n",
      "* There is also the third-party [`Beautiful Soup`](http://www.crummy.com/software/BeautifulSoup/) library\n",
      "  - One of the big features of Beautiful Soup is that it handles malformed HTML intelligently.\n",
      "  - It also handles encodings explicitly. If you've ever worked with Unicode, you will appreciate this.\n",
      "  - It leverages `lxml` and `html5lib`\n",
      "* That said, we are going to use `lxml`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We have our page, but how to get the links we want?\n",
      "* Pop over to [the page](http://en.wikipedia.org/wiki/Cardinal_electors_in_Papal_conclave,_2005) in your browser.\n",
      "* In Chrome, you can hit F12 to bring up the developer tools.\n",
      "* In Firefox, you will probably want to install [Firebug](http://getfirebug.com/) first. F12 will open Firebug.\n",
      "* In Chrome, click on Elements and hover over the HTML until you find the part that contains the source you want.\n",
      "* Right-click and `Copy XPath`\n",
      "  - The result wil be something like `\"//*[@id=\"mw-content-text\"]/ol[1]\"`\n",
      "* In Firefox, click on HTML and do the same though you will select `Copy XPath`\n",
      "  - The result will be something like `\"/html/body/div[3]/div[3]/div[4]/ol\"`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* What in the world is this?\n",
      "* [XML](http://en.wikipedia.org/wiki/XML), or extensible markup language, defines a set of rules for making documents both machine and human readable.\n",
      "  - Many, many XML-based languages exist. This includes XHTML/HTML5.\n",
      "* [XPath](http://www.w3schools.com/xpath/xpath_syntax.asp) provides a way to select nodes in an XML document.\n",
      "* Unpacking `\"//*[@id=\"mw-content-text\"]/ol[1]\"`\n",
      "  - `//` means select nodes that are either self or a descedant anywhere from self.\n",
      "  - `*` is the wildcard and matches any element node\n",
      "  - [@] selects attributes of nodes\n",
      "  - [1] denotes selecting the first element\n",
      "  - So this says get any node with id=\"mw-content-text\" and grab it's first `<ol>` node.\n",
      "* Unpacking `\"/html/body/div[3]/div[3]/div[4]/ol\"`\n",
      "  - This is a bit clearer if you're familiar with html.\n",
      "  - These are just HTML tags in order of descendancy\n",
      "  - Note that this will grab every ol element at that level\n",
      "\n",
      "Example:\n",
      "    \n",
      "    <html>\n",
      "    <body>\n",
      "      <div>...</div>\n",
      "      <div>...</div>\n",
      "      <div>\n",
      "        <div>...</div>\n",
      "        ...\n",
      "          ...\n",
      "          <div>\n",
      "            <ol>GRAB THIS</ol>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xml_tree = html.parse(page)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tables = xml_tree.xpath(\"/html/body/div[3]/div[3]/div[4]/ol\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, I usually use trial and error, go back to the browser and look at the HTML source, or use the additional information in the developer tools to identify what I need."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tables"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table = tables[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "row = table.xpath(\"li\")[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "link = row.xpath(\"a[1]\")[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "link.text, link.get(\"href\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "electors = []\n",
      "\n",
      "for table in tables:\n",
      "    for row in table.xpath('li'):\n",
      "        # use star because of <b><a>\n",
      "        link = row.xpath(\"a[1]|*/a[1]\")[0]\n",
      "        link.make_links_absolute()\n",
      "        name = link.text\n",
      "        href = link.get(\"href\")\n",
      "        electors.append(dict(name=name, url=href))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert len(electors) == 115"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Wikipedia API and JSON"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We now have the name and links\n",
      "* We can get the information we really need using the [Wikipedia JSON API](http://en.wikipedia.org/w/api.php)\n",
      "* JSON is a text-based open standard for data exchange based on Javascript data structures\n",
      "  - It's an alternative to xml that's a bit easier on the eyes in my opinon"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib import urlencode\n",
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "WIKI_API_URL = \"http://en.wikipedia.org/w/api.php\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "request = urllib2.Request(WIKI_API_URL)\n",
      "request.add_header(\"User-Agent\", \n",
      "                   \"WikiApiDemo/0.0 +http://jseabold.net\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "request_params = {\n",
      "        \"action\" : \"parse\",\n",
      "        \"format\" : \"json\",\n",
      "        \"redirects\" : \"true\",\n",
      "         }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print urlencode(request_params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take one example to be clear"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name, url = electors[0][\"name\"], electors[0][\"url\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need the end of the URL."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = url.rsplit(\"/\")[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Update the `request_params` dict with the target `page`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "request_params.update({\"page\" : url})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = urlencode(request_params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "request = urllib2.Request(WIKI_API_URL, data)\n",
      "page = opener.open(request)\n",
      "json_page = page.read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print json_page[:200]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json_obj = json.loads(json_page)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(json_obj)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json_obj.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json_obj[\"parse\"].keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = json_obj[\"parse\"][\"text\"]\n",
      "text = text[\"*\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Before we were able to parse the returned file-like object from `opener.open`.\n",
      "* Now we need to parse a string. \n",
      "* These strings contain non-ascii characters (utf-8 in this case)\n",
      "* So we make a parser"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parser = html.HTMLParser(encoding=\"utf-8\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "html_tree = etree.HTML(text, parser=parser)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = html_tree.text_content()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print text[:480]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's go ahead and get them all, if we haven't already."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We need to be aware of one thing. \n",
      "* URLs like `Juli\u00e1n_Herranz_Casado` will be replaced with the URL encoded `Juli%C3%A1n_Herranz_Casado`.\n",
      "* We need to first `unquote` these before passing them to `urlencode`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib2 import unquote"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print unquote('Juli%C3%A1n_Herranz_Casado')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not os.path.exists(\"./ElectorData05.json\"):\n",
      "    for elector in electors:\n",
      "        name, url = elector[\"name\"], elector[\"url\"]\n",
      "        url = url.rsplit(\"/\")[-1]\n",
      "        request_params.update({\"page\" : unquote(url)})\n",
      "        data = urlencode(request_params)\n",
      "        request = urllib2.Request(WIKI_API_URL, data)\n",
      "        page = opener.open(request)\n",
      "        json_obj = json.loads(page.read())\n",
      "        text = json_obj[\"parse\"][\"text\"][\"*\"]\n",
      "        html_tree = etree.HTML(text, parser=parser)\n",
      "        text = html_tree.text_content()\n",
      "        elector[\"text\"] = text\n",
      "else:\n",
      "    electors = json.load(open(\"./ElectorData05.json\", \"r\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save the data to disk using json. You could use the `pickle` module if you wanted to use a binary format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"ElectorData05.json\", \"wb\") as json_out:\n",
      "    json.dump(electors, json_out)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What Now?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Clean the text and compute n-grams\n",
      "* Calculate TF-IDF\n",
      "* Decide on a vocabularly\n",
      "* Quantify text vectors\n",
      "* Compute distance metrics between the cardinals based on wiki page bios\n",
      "* Formulate a model for prediction for 2005 Conclave given we know the outcome\n",
      "* Forecast 2013 voting\n",
      "* We may explore this model in more detail in CSC-432, if there's sufficient interest."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cleaning Text"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Regular Expressions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* A regular expression, or regex, is a text string that describes a search pattern\n",
      "* Python has the [re](http://docs.python.org/2/library/re.html) module.\n",
      "* There is also the third-party [regex](https://pypi.python.org/pypi/regex) module\n",
      "  - This library has advantages if you need to work with Unicode character categories\n",
      "  - If we were properly to clean the Wikipedia pages, we would need to remove Unicode punctuation\n",
      "  - Apparently this is supposed to become part of the standard library, but I don't know the status of this\n",
      "* Let's look at some examples to get the idea of regex"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "A Simple Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The simplest regular expressions are plain characters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1 = \"Moby Dick by Herman Melville\"\n",
      "text2 = \"Vaudeville\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Matches return a match object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"ville\", text1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search = re.search(\"ville\", text1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Match objects have a group method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"ville\", text2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Non-matches return `None`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"Maude\", \"Vaudeville\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "A little more complicated"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regular expressions may also contain special characters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text3 = \"manners\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4 = \"mandolin\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The character `\\w` is a special character that matches any alphanumeric character and the underscore."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"man\\w\", text1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"man\\w\", text3).group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"man\\w\", text4).group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another special character is `.` which matches any character except a newline character."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"man.\", text1).group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"man.\", text3).group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can escape the special characters with a `\\`, so that `\\.` would match a period."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"street\\.\", \"Main street.\").group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`[]` is used to indicate a set of characters. For instances `[amk]` will match a, m, or k."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"Mar[atk]\", \"Mart\").group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"Mar[atk]\", \"Mark\").group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"Mar[atk]\", \"Mara\").group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search(\"Mar[atk]\", \"Mars\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use what we know so far to strip out punctuation using the `re.sub` function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pattern = \"[%s]\" % re.escape(\".,'\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print pattern"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.sub(pattern, \"\", \"Hello, ma'am.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also make sure we strip out unicode by using the `flags` argument. For example, our first text has unicode spaces (`\\xa0`) in it. The special character for whitespace is `\\s` this matches any whitespace character including `[ \\t\\n\\r\\f\\v]`. With the `UNICODE` flag set it also matches any character designated as whitespace in the Unicode character properties database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "electors[0][\"text\"][:64]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.sub(\"\\s\", \" \", electors[0][\"text\"][:64], flags=re.UNICODE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last special character we will want to know about is `+`. `+` matches 1 or more of the preceding characters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.sub(\" +\", \" \", \"This   has      a lot of    repeated spaces\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Now we're going to use this to make a function to strip out all of the punctation from our text. \n",
      "* We can get all of the ASCII punctuations from the `string` module."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print string.punctuation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "punc_pattern = \"[%s]\" % re.escape(string.punctuation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "space_pattern = \"\\s+\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_text(text, punc_pattern, space_pattern):\n",
      "    text = re.sub(punc_pattern, \"\", text)\n",
      "    text = re.sub(space_pattern, \" \", text, flags=re.UNICODE)\n",
      "    text = re.sub(\"\\d+\", \"\", text)\n",
      "    # some cruft I saw ex post\n",
      "    # could use regex to get unicode across the board\n",
      "    text = re.sub(u\"\\u2014\", \"\", text)\n",
      "    return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for elector in electors:\n",
      "    text = clean_text(elector[\"text\"], punc_pattern, space_pattern)\n",
      "    elector.update({\"text\" : text})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Remove Stop Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dates = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\",\n",
      "         \"september\", \"october\", \"november\", \"december\", \"monday\", \"tuesday\", \n",
      "         \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\n",
      "\n",
      "try:\n",
      "    from nltk.corpus import stopwords\n",
      "    all_stopwords = stopwords.words(\"english\") \n",
      "except:\n",
      "    all_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
      "        'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
      "         'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', \n",
      "         'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
      "         'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', \n",
      "         'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
      "         'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
      "         'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
      "         'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
      "         'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
      "         'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
      "         'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
      "         'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'edit',\n",
      "         'disambiguation']\n",
      "all_stopwords += [\"edit\", \"disambiguation\"] \n",
      "all_stopwords += dates"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_stop_words(x, stop_words):\n",
      "    \"\"\"\n",
      "    Creates a regex for all the stop words and then removes them from the text.\n",
      "\n",
      "    \\b matches the empty string but only at the beginning and end of words.\n",
      "    \"\"\"\n",
      "    stop_words_regex = r'\\b%s\\b' % r'\\b|\\b'.join(stop_words)\n",
      "    x = re.sub(stop_words_regex, \"\", x, flags=re.UNICODE)\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def count(text):\n",
      "    \"\"\"\n",
      "    Generator that yields the unigrams, bigrams, and trigrams in a single\n",
      "    space separated string from a given text.\n",
      "    \"\"\"\n",
      "    words = text.split()\n",
      "    for i in range(len(words)-2):\n",
      "        yield words[i], 1\n",
      "        yield ' '.join(words[i:i+2]), 1 \n",
      "        yield ' '.join(words[i:i+3]), 1\n",
      "    # yield that last bi-gram and last 2 uni-grams\n",
      "    if len(words) > 1:\n",
      "        yield ' '.join(words[-2:]), 1\n",
      "        yield words[-2], 1\n",
      "        yield words[-1], 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import groupby\n",
      "import operator\n",
      "\n",
      "def key(k_v):\n",
      "    return k_v[0]\n",
      "\n",
      "def kvgroup(kviter):\n",
      "    \"\"\"\n",
      "    Copy of the function from disco.utils\n",
      "\n",
      "    Group the values of consecutive keys which compare equal.\n",
      "\n",
      "    Takes an iterator over ``k, v`` pairs,\n",
      "    and returns an iterator over ``k, vs``.\n",
      "    Does not sort the input first.\n",
      "    \"\"\"\n",
      "    for k, kvs in groupby(kviter, key):\n",
      "        yield k, (v for _k, v in kvs)\n",
      "        \n",
      "def combine(counts):\n",
      "    for word, count in kvgroup(sorted(counts)):\n",
      "        yield word, sum(count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for elector in electors:\n",
      "    text = elector[\"text\"]\n",
      "    text = text.lower()\n",
      "    cleaned_text = remove_stop_words(text, all_stopwords)\n",
      "    elector[\"text_vector\"] = dict(i for i in combine(count(cleaned_text)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "elector[\"text_vector\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jaccard_similarity(vector1, vector2):\n",
      "    '''\n",
      "    Compute Jaccard Similarity between two sparse vectors,\n",
      "    represented as dicts.\n",
      "    '''\n",
      "    # checking for set membership is fast\n",
      "    keys1 = set(vector1.keys())\n",
      "    keys2 = set(vector2.keys())\n",
      "    all_keys = keys1.union(keys)\n",
      "    union = len(all_keys)\n",
      "    intersection = 0.\n",
      "    for key in all_keys:\n",
      "        if key in vector1 and key in vector2:\n",
      "            intersection += 1.\n",
      "    return intersection / union"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}